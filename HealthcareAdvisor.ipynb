{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "752b0e71-5544-4c2d-982e-d316f029e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded symptoms: 1000 rows from symptoms.csv\n",
      "Loaded allergies: 600 rows from allergies.csv\n",
      "Loaded careplans: 600 rows from careplans.csv\n",
      "Loaded conditions: 600 rows from conditions.csv\n",
      "Loaded immunizations: 600 rows from immunizations.csv\n",
      "Loaded observations: 600 rows from observations.csv\n",
      "Loaded procedures: 600 rows from procedures.csv\n",
      "Loaded medicine: 300 rows from medicine_disease.csv\n",
      "Loaded herbal: 300 rows from herbaltreatment_disease.csv\n",
      "Loaded nutrition: 300 rows from nutrition_disease.csv\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and load datasets\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load datasets\n",
    "paths = {\n",
    "    'symptoms': 'symptoms.csv',\n",
    "    'allergies': 'allergies.csv',\n",
    "    'careplans': 'careplans.csv',\n",
    "    'conditions': 'conditions.csv',\n",
    "    'immunizations': 'immunizations.csv',\n",
    "    'observations': 'observations.csv',\n",
    "    'procedures': 'procedures.csv',\n",
    "    'medicine': 'medicine_disease.csv',\n",
    "    'herbal': 'herbaltreatment_disease.csv',\n",
    "    'nutrition': 'nutrition_disease.csv',\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for name, p in paths.items():\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            datasets[name] = pd.read_csv(p)\n",
    "            print(f\"Loaded {name}: {datasets[name].shape[0]} rows from {p}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {name} from {p}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: {name} file not found at {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4629299c-2bf8-4f9d-acce-543a9c3acee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def clean_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17b18ff9-7ef1-4395-a7d3-96eec7e1ab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Added 1000 symptom-disease mappings\n",
      "âœ“ Added 600 clinical observations\n",
      "âœ“ Added 600 medical conditions\n",
      "âœ“ Added 600 care plans\n",
      "âœ“ Added 600 allergy entries\n",
      "\n",
      "ðŸ” Performing corpus optimization...\n",
      "Removed 1851 duplicates\n",
      "\n",
      "ðŸŽ¯ FINAL CORPUS BUILT: 1549 total entries\n",
      "Corpus composition:\n",
      "  symptoms: 1000 entries\n",
      "  observation: 8 entries\n",
      "  condition: 525 entries\n",
      "  careplan: 8 entries\n",
      "  allergy: 8 entries\n",
      "\n",
      "ðŸ§  Training TF-IDF model on 1549 unique entries...\n",
      "âœ“ TF-IDF vocabulary size: 1453 terms\n"
     ]
    }
   ],
   "source": [
    "#  Build matching corpus \n",
    "corpus = []\n",
    "\n",
    "# === PRIORITY 1: SYMPTOMS.CSV (Primary - Most Valuable) ===\n",
    "if 'symptoms' in datasets:\n",
    "    sym_df = datasets['symptoms']\n",
    "    disease_col = 'Disease'\n",
    "    symptoms_col = 'Symptoms'\n",
    "    \n",
    "    symptom_entries = 0\n",
    "    for i, r in sym_df.iterrows():\n",
    "        disease = str(r.get(disease_col, '')).strip()\n",
    "        symptoms = str(r.get(symptoms_col, '')).strip()\n",
    "        \n",
    "        if disease and symptoms:\n",
    "            # Clean and format symptoms\n",
    "            clean_symptoms = symptoms.replace(';', ' ').replace(',', ' ')\n",
    "            text = f\"symptoms: {clean_symptoms}\"\n",
    "            corpus.append({\n",
    "                'id': f'sym_{i}', \n",
    "                'type': 'symptoms', \n",
    "                'text': text, \n",
    "                'diagnosis': disease,\n",
    "                'symptoms': clean_symptoms,\n",
    "                'priority': 1\n",
    "            })\n",
    "            symptom_entries += 1\n",
    "    print(f\"âœ“ Added {symptom_entries} symptom-disease mappings\")\n",
    "\n",
    "# === PRIORITY 2: OBSERVATIONS (Clinical Context) ===\n",
    "if 'observations' in datasets:\n",
    "    obs_df = datasets['observations']\n",
    "    obs_col = next((c for c in obs_df.columns if 'obser' in c.lower() or 'symptom' in c.lower() or 'description' in c.lower()), obs_df.columns[0])\n",
    "    cause_col = next((c for c in obs_df.columns if 'cause' in c.lower() or 'possible' in c.lower() or 'diagnosis' in c.lower()), None)\n",
    "    \n",
    "    observation_entries = 0\n",
    "    for i, r in obs_df.iterrows():\n",
    "        obs = str(r.get(obs_col, '')).strip()\n",
    "        cause = str(r.get(cause_col, '')).strip() if cause_col else ''\n",
    "        \n",
    "        if obs:\n",
    "            text = f\"observation: {obs}\"\n",
    "            if cause:\n",
    "                text += f\" possible_cause: {cause}\"\n",
    "            \n",
    "            diagnosis = cause if cause else obs\n",
    "            corpus.append({\n",
    "                'id': f'obs_{i}', \n",
    "                'type': 'observation', \n",
    "                'text': text, \n",
    "                'diagnosis': diagnosis,\n",
    "                'observation': obs,\n",
    "                'cause': cause,\n",
    "                'priority': 2\n",
    "            })\n",
    "            observation_entries += 1\n",
    "    print(f\"âœ“ Added {observation_entries} clinical observations\")\n",
    "\n",
    "# === PRIORITY 3: CONDITIONS (Structured Medical Knowledge) ===\n",
    "if 'conditions' in datasets:\n",
    "    cond_df = datasets['conditions']\n",
    "    cond_col = next((c for c in cond_df.columns if 'cond' in c.lower() or 'condition' in c.lower() or 'diagnosis' in c.lower()), cond_df.columns[0])\n",
    "    \n",
    "    # Look for additional context columns\n",
    "    desc_col = next((c for c in cond_df.columns if 'desc' in c.lower() or 'description' in c.lower()), None)\n",
    "    severity_col = next((c for c in cond_df.columns if 'sever' in c.lower()), None)\n",
    "    age_col = next((c for c in cond_df.columns if 'age' in c.lower()), None)\n",
    "    gender_col = next((c for c in cond_df.columns if 'gender' in c.lower()), None)\n",
    "    \n",
    "    condition_entries = 0\n",
    "    for i, r in cond_df.iterrows():\n",
    "        condition = str(r.get(cond_col, '')).strip()\n",
    "        description = str(r.get(desc_col, '')).strip() if desc_col else ''\n",
    "        severity = str(r.get(severity_col, '')).strip() if severity_col else ''\n",
    "        age = str(r.get(age_col, '')).strip() if age_col else ''\n",
    "        gender = str(r.get(gender_col, '')).strip() if gender_col else ''\n",
    "        \n",
    "        if condition:\n",
    "            text_parts = [f\"condition: {condition}\"]\n",
    "            if description:\n",
    "                text_parts.append(f\"description: {description}\")\n",
    "            if severity:\n",
    "                text_parts.append(f\"severity: {severity}\")\n",
    "            if age:\n",
    "                text_parts.append(f\"age: {age}\")\n",
    "            if gender:\n",
    "                text_parts.append(f\"gender: {gender}\")\n",
    "            \n",
    "            text = \" \".join(text_parts)\n",
    "            corpus.append({\n",
    "                'id': f'cond_{i}', \n",
    "                'type': 'condition', \n",
    "                'text': text, \n",
    "                'diagnosis': condition,\n",
    "                'description': description,\n",
    "                'priority': 3\n",
    "            })\n",
    "            condition_entries += 1\n",
    "    print(f\"âœ“ Added {condition_entries} medical conditions\")\n",
    "\n",
    "# === PRIORITY 4: CAREPLANS (Treatment Context) ===\n",
    "if 'careplans' in datasets:\n",
    "    cp_df = datasets['careplans']\n",
    "    diag_col = next((c for c in cp_df.columns if 'diag' in c.lower() or 'condition' in c.lower()), cp_df.columns[0])\n",
    "    plan_col = next((c for c in cp_df.columns if 'care' in c.lower() or 'plan' in c.lower() or 'treatment' in c.lower()), None)\n",
    "    goal_col = next((c for c in cp_df.columns if 'goal' in c.lower() or 'objective' in c.lower()), None)\n",
    "    \n",
    "    careplan_entries = 0\n",
    "    for i, r in cp_df.iterrows():\n",
    "        diag = str(r.get(diag_col, '')).strip()\n",
    "        plan = str(r.get(plan_col, '')).strip() if plan_col else ''\n",
    "        goal = str(r.get(goal_col, '')).strip() if goal_col else ''\n",
    "        \n",
    "        if diag:\n",
    "            text_parts = [f\"diagnosis: {diag}\"]\n",
    "            if plan:\n",
    "                text_parts.append(f\"care_plan: {plan}\")\n",
    "            if goal:\n",
    "                text_parts.append(f\"treatment_goal: {goal}\")\n",
    "            \n",
    "            text = \" \".join(text_parts)\n",
    "            corpus.append({\n",
    "                'id': f'cp_{i}', \n",
    "                'type': 'careplan', \n",
    "                'text': text, \n",
    "                'diagnosis': diag,\n",
    "                'care_plan': plan,\n",
    "                'priority': 4\n",
    "            })\n",
    "            careplan_entries += 1\n",
    "    print(f\"âœ“ Added {careplan_entries} care plans\")\n",
    "\n",
    "# === PRIORITY 5: ALLERGIES (Additional Medical Context) ===\n",
    "if 'allergies' in datasets:\n",
    "    allergy_df = datasets['allergies']\n",
    "    allergy_col = next((c for c in allergy_df.columns if 'aller' in c.lower() or 'reaction' in c.lower()), allergy_df.columns[0])\n",
    "    severity_col = next((c for c in allergy_df.columns if 'sever' in c.lower()), None)\n",
    "    \n",
    "    allergy_entries = 0\n",
    "    for i, r in allergy_df.iterrows():\n",
    "        allergy = str(r.get(allergy_col, '')).strip()\n",
    "        severity = str(r.get(severity_col, '')).strip() if severity_col else ''\n",
    "        \n",
    "        if allergy:\n",
    "            text = f\"allergy: {allergy}\"\n",
    "            if severity:\n",
    "                text += f\" severity: {severity}\"\n",
    "            \n",
    "            corpus.append({\n",
    "                'id': f'allergy_{i}', \n",
    "                'type': 'allergy', \n",
    "                'text': text, \n",
    "                'diagnosis': f\"Allergy: {allergy}\",\n",
    "                'priority': 5\n",
    "            })\n",
    "            allergy_entries += 1\n",
    "    print(f\"âœ“ Added {allergy_entries} allergy entries\")\n",
    "\n",
    "# === DEDUPLICATION AND QUALITY CONTROL ===\n",
    "print(\"\\nðŸ” Performing corpus optimization...\")\n",
    "\n",
    "# Remove exact duplicates\n",
    "initial_count = len(corpus)\n",
    "seen_texts = set()\n",
    "unique_corpus = []\n",
    "\n",
    "for item in corpus:\n",
    "    clean_t = clean_text(item['text'])\n",
    "    if clean_t not in seen_texts and clean_t.strip():\n",
    "        seen_texts.add(clean_t)\n",
    "        unique_corpus.append(item)\n",
    "\n",
    "corpus = unique_corpus\n",
    "print(f\"Removed {initial_count - len(corpus)} duplicates\")\n",
    "\n",
    "# Sort by priority (higher priority first)\n",
    "corpus.sort(key=lambda x: x.get('priority', 999))\n",
    "\n",
    "# === FALLBACK: Ensure we always have some data ===\n",
    "if len(corpus) == 0:\n",
    "    print(\"âš ï¸  No datasets found - creating fallback entries\")\n",
    "    corpus = [\n",
    "        {\n",
    "            'id': 'fallback_1', 'type': 'fallback', 'priority': 999,\n",
    "            'text': 'symptoms: fever cough headache fatigue', \n",
    "            'diagnosis': 'Viral Infection'\n",
    "        },\n",
    "        {\n",
    "            'id': 'fallback_2', 'type': 'fallback', 'priority': 999,\n",
    "            'text': 'symptoms: sore throat runny nose sneezing', \n",
    "            'diagnosis': 'Common Cold'\n",
    "        },\n",
    "        {\n",
    "            'id': 'fallback_3', 'type': 'fallback', 'priority': 999,\n",
    "            'text': 'symptoms: chest pain shortness of breath', \n",
    "            'diagnosis': 'Respiratory Issue'\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# === FINAL CORPUS STATISTICS ===\n",
    "print(f\"\\nðŸŽ¯ FINAL CORPUS BUILT: {len(corpus)} total entries\")\n",
    "type_counts = {}\n",
    "for item in corpus:\n",
    "    t = item['type']\n",
    "    type_counts[t] = type_counts.get(t, 0) + 1\n",
    "\n",
    "print(\"Corpus composition:\")\n",
    "for type_name, count in type_counts.items():\n",
    "    print(f\"  {type_name}: {count} entries\")\n",
    "\n",
    "# Create DataFrame and prepare for TF-IDF\n",
    "corpus_df = pd.DataFrame(corpus)\n",
    "corpus_df['clean_text'] = corpus_df['text'].apply(clean_text)\n",
    "\n",
    "print(f\"\\nðŸ§  Training TF-IDF model on {len(corpus_df)} unique entries...\")\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', min_df=2)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus_df['clean_text'])\n",
    "print(f\"âœ“ TF-IDF vocabulary size: {len(vectorizer.vocabulary_)} terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "842f7079-537e-4d5c-8b78-601e196e04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Diagnosis matcher function\n",
    "def match_diagnoses(symptom_text, top_k=5):\n",
    "    s = clean_text(symptom_text)\n",
    "    if not s or tfidf_matrix is None:\n",
    "        return []\n",
    "    v = vectorizer.transform([s])\n",
    "    sims = cosine_similarity(v, tfidf_matrix).flatten()\n",
    "    score_by_diag = defaultdict(float)\n",
    "    count_by_diag = defaultdict(int)\n",
    "    for idx, sim in enumerate(sims):\n",
    "        diag = corpus_df.loc[idx, 'diagnosis'] or 'Unknown'\n",
    "        score_by_diag[diag] += float(sim)\n",
    "        count_by_diag[diag] += 1\n",
    "    results = []\n",
    "    for diag, score in score_by_diag.items():\n",
    "        avg_score = score / max(1, count_by_diag[diag])\n",
    "        results.append((diag, avg_score))\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    filtered = [r for r in results if r[1] > 0.01]\n",
    "    return filtered[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f7ced8c-b694-4d61-a5d0-c6712802f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Lookup and recommendation functions\n",
    "def lookup_table(df, key_col, value_col, diagnosis):\n",
    "    if df is None or diagnosis is None:\n",
    "        return []\n",
    "    \n",
    "    diag_l = diagnosis.strip().lower()\n",
    "    results = []\n",
    "    \n",
    "    # Try exact match first\n",
    "    if key_col in df.columns:\n",
    "        exact_matches = df[df[key_col].astype(str).str.lower() == diag_l]\n",
    "        if len(exact_matches) > 0:\n",
    "            results.extend(exact_matches[value_col].dropna().astype(str).tolist())\n",
    "    \n",
    "    # Try partial matches if no exact matches found\n",
    "    if not results and key_col in df.columns:\n",
    "        # Split diagnosis into words and try to match any part\n",
    "        diag_words = diag_l.split()\n",
    "        for word in diag_words:\n",
    "            if len(word) > 3:  # Only search for words longer than 3 characters\n",
    "                contains_matches = df[df[key_col].astype(str).str.lower().str.contains(word, na=False)]\n",
    "                if len(contains_matches) > 0:\n",
    "                    results.extend(contains_matches[value_col].dropna().astype(str).tolist())\n",
    "    \n",
    "    # Fallback: search any column for the diagnosis\n",
    "    if not results:\n",
    "        for c in df.columns:\n",
    "            if c != value_col:  # Don't search the value column itself\n",
    "                subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
    "                if len(subset) > 0:\n",
    "                    results.extend(subset[value_col].dropna().astype(str).tolist())\n",
    "                    break\n",
    "    \n",
    "    # Return unique results\n",
    "    return list(dict.fromkeys(results))\n",
    "\n",
    "def recommend_medicines(diagnosis, top_n=5):\n",
    "    df = datasets.get('medicine')\n",
    "    if df is None:\n",
    "        print(\"Medicine dataset not available\")\n",
    "        return []\n",
    "    \n",
    "    # More flexible column detection\n",
    "    key_col = next((c for c in df.columns if any(x in c.lower() for x in ['disease', 'condition', 'diagnosis', 'illness', 'disorder'])), df.columns[0])\n",
    "    val_col = next((c for c in df.columns if any(x in c.lower() for x in ['medicine', 'medication', 'drug', 'treatment', 'prescription', 'therapy'])), None)\n",
    "    \n",
    "    if val_col is None:\n",
    "        # If no obvious value column, use the second column or first if only one column\n",
    "        val_col = df.columns[1] if df.shape[1] > 1 else df.columns[0]\n",
    "    \n",
    "    print(f\"Medicine lookup: diagnosis='{diagnosis}', key_col='{key_col}', val_col='{val_col}'\")\n",
    "    results = lookup_table(df, key_col, val_col, diagnosis)\n",
    "    print(f\"Found medicine results: {results}\")\n",
    "    return results[:top_n]\n",
    "\n",
    "def recommend_herbal(diagnosis, top_n=5):\n",
    "    df = datasets.get('herbal')\n",
    "    if df is None:\n",
    "        print(\"Herbal dataset not available\")\n",
    "        return []\n",
    "    \n",
    "    key_col = next((c for c in df.columns if any(x in c.lower() for x in ['disease', 'condition', 'diagnosis'])), df.columns[0])\n",
    "    val_col = next((c for c in df.columns if any(x in c.lower() for x in ['herbal', 'remedy', 'treatment', 'plant', 'natural'])), None)\n",
    "    \n",
    "    if val_col is None:\n",
    "        val_col = df.columns[1] if df.shape[1] > 1 else df.columns[0]\n",
    "    \n",
    "    print(f\"Herbal lookup: diagnosis='{diagnosis}', key_col='{key_col}', val_col='{val_col}'\")\n",
    "    results = lookup_table(df, key_col, val_col, diagnosis)\n",
    "    print(f\"Found herbal results: {results}\")\n",
    "    return results[:top_n]\n",
    "\n",
    "def recommend_nutrition(diagnosis, top_n=5):\n",
    "    df = datasets.get('nutrition')\n",
    "    if df is None:\n",
    "        print(\"Nutrition dataset not available\")\n",
    "        return []\n",
    "    \n",
    "    key_col = next((c for c in df.columns if any(x in c.lower() for x in ['disease', 'condition', 'diagnosis'])), df.columns[0])\n",
    "    val_col = next((c for c in df.columns if any(x in c.lower() for x in ['nutrition', 'food', 'diet', 'supplement', 'vitamin'])), None)\n",
    "    \n",
    "    if val_col is None:\n",
    "        val_col = df.columns[1] if df.shape[1] > 1 else df.columns[0]\n",
    "    \n",
    "    print(f\"Nutrition lookup: diagnosis='{diagnosis}', key_col='{key_col}', val_col='{val_col}'\")\n",
    "    results = lookup_table(df, key_col, val_col, diagnosis)\n",
    "    print(f\"Found nutrition results: {results}\")\n",
    "    return results[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b82326f-0ff2-4240-8fab-4eb01da51da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Lifestyle recommendations functions\n",
    "def extract_lifestyle_from_careplan(careplan_text):\n",
    "    lifestyle_terms = []\n",
    "    text_lower = careplan_text.lower()\n",
    "    \n",
    "    if 'diet' in text_lower:\n",
    "        if 'modification' in text_lower:\n",
    "            lifestyle_terms.append('Follow dietary modifications as prescribed')\n",
    "        else:\n",
    "            lifestyle_terms.append('Maintain healthy diet')\n",
    "    \n",
    "    if 'exercise' in text_lower or 'physical activity' in text_lower:\n",
    "        lifestyle_terms.append('Regular physical activity as recommended')\n",
    "    \n",
    "    if 'rehabilitation' in text_lower:\n",
    "        lifestyle_terms.append('Participate in recommended rehabilitation program')\n",
    "    \n",
    "    if 'therapy' in text_lower and 'behavioral' in text_lower:\n",
    "        lifestyle_terms.append('Engage in behavioral therapy sessions')\n",
    "    \n",
    "    if 'monitoring' in text_lower:\n",
    "        lifestyle_terms.append('Regular self-monitoring as advised')\n",
    "    \n",
    "    return lifestyle_terms if lifestyle_terms else ['Follow prescribed care plan']\n",
    "\n",
    "def get_generic_lifestyle_recommendations(diagnosis):\n",
    "    diag = (diagnosis or '').lower()\n",
    "    recs = []\n",
    "    \n",
    "    if any(x in diag for x in ['migraine', 'headache']):\n",
    "        recs += [\n",
    "            'Identify and avoid headache triggers',\n",
    "            'Maintain regular sleep schedule',\n",
    "            'Stay hydrated',\n",
    "            'Manage stress through relaxation techniques'\n",
    "        ]\n",
    "    elif 'hypertension' in diag or 'high blood pressure' in diag:\n",
    "        recs += [\n",
    "            'Reduce salt intake',\n",
    "            'Maintain healthy weight',\n",
    "            'Limit alcohol consumption',\n",
    "            'Regular aerobic exercise'\n",
    "        ]\n",
    "    elif 'diabetes' in diag:\n",
    "        recs += [\n",
    "            'Monitor blood glucose regularly',\n",
    "            'Reduce refined carbohydrates and sugary drinks',\n",
    "            'Increase physical activity'\n",
    "        ]\n",
    "    elif 'asthma' in diag:\n",
    "        recs += [\n",
    "            'Avoid known triggers (smoke, pollen, dust)',\n",
    "            'Follow inhaler/medication plan',\n",
    "            'Consider allergen-proofing home'\n",
    "        ]\n",
    "    elif 'heart' in diag:\n",
    "        recs += [\n",
    "            'Cardiac-friendly diet low in saturated fats',\n",
    "            'Regular moderate exercise',\n",
    "            'Stress management techniques'\n",
    "        ]\n",
    "    elif 'kidney' in diag:\n",
    "        recs += [\n",
    "            'Follow renal diet restrictions',\n",
    "            'Monitor fluid intake',\n",
    "            'Regular kidney function tests'\n",
    "        ]\n",
    "    elif 'obesity' in diag:\n",
    "        recs += [\n",
    "            'Calorie-controlled balanced diet',\n",
    "            'Regular physical activity',\n",
    "            'Behavior modification for eating habits'\n",
    "        ]\n",
    "    elif 'depression' in diag:\n",
    "        recs += [\n",
    "            'Regular sleep schedule',\n",
    "            'Social engagement and support',\n",
    "            'Mindfulness and relaxation techniques'\n",
    "        ]\n",
    "    elif 'infection' in diag or 'fever' in diag:\n",
    "        recs += [\n",
    "            'Stay hydrated',\n",
    "            'Rest and seek medical evaluation if symptoms worsen'\n",
    "        ]\n",
    "    \n",
    "    general_recs = ['Maintain balanced diet', 'Regular exercise', 'Get routine checkups']\n",
    "    all_recs = recs + general_recs\n",
    "    return list(dict.fromkeys(all_recs))[:5]\n",
    "\n",
    "def lifestyle_recommendations(diagnosis):\n",
    "    if 'careplans' not in datasets:\n",
    "        return get_generic_lifestyle_recommendations(diagnosis)\n",
    "    \n",
    "    cp_df = datasets['careplans']\n",
    "    diag_col = next((c for c in cp_df.columns if 'diag' in c.lower()), cp_df.columns[0])\n",
    "    plan_col = next((c for c in cp_df.columns if 'care' in c.lower() or 'plan' in c.lower()), None)\n",
    "    \n",
    "    if plan_col is None:\n",
    "        return get_generic_lifestyle_recommendations(diagnosis)\n",
    "    \n",
    "    diag_l = (diagnosis or '').strip().lower()\n",
    "    \n",
    "    # Try exact match first\n",
    "    exact_matches = cp_df[cp_df[diag_col].astype(str).str.lower() == diag_l]\n",
    "    if len(exact_matches) > 0:\n",
    "        plans = exact_matches[plan_col].dropna().astype(str).tolist()\n",
    "        if plans:\n",
    "            lifestyle_advice = []\n",
    "            for plan in plans:\n",
    "                lifestyle_advice.extend(extract_lifestyle_from_careplan(plan))\n",
    "            \n",
    "            if lifestyle_advice:\n",
    "                return list(dict.fromkeys(lifestyle_advice))[:5]\n",
    "    \n",
    "    # Try partial match\n",
    "    for word in diag_l.split():\n",
    "        if len(word) > 4:\n",
    "            partial_matches = cp_df[cp_df[diag_col].astype(str).str.lower().str.contains(word, na=False)]\n",
    "            if len(partial_matches) > 0:\n",
    "                plans = partial_matches[plan_col].dropna().astype(str).tolist()\n",
    "                if plans:\n",
    "                    lifestyle_advice = []\n",
    "                    for plan in plans:\n",
    "                        lifestyle_advice.extend(extract_lifestyle_from_careplan(plan))\n",
    "                    \n",
    "                    if lifestyle_advice:\n",
    "                        return list(dict.fromkeys(lifestyle_advice))[:5]\n",
    "    \n",
    "    return get_generic_lifestyle_recommendations(diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89225e64-2b4e-4538-8743-86fe8bb4aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compose advice function\n",
    "def compose_advice(diagnosis, score):\n",
    "    meds = recommend_medicines(diagnosis)\n",
    "    herbs = recommend_herbal(diagnosis)\n",
    "    nuts = recommend_nutrition(diagnosis)\n",
    "    life = lifestyle_recommendations(diagnosis)\n",
    "    lines = []\n",
    "    lines.append(f\"Most likely diagnosis: {diagnosis} (score={score:.3f})\")\n",
    "    if meds:\n",
    "        lines.append(\"Recommended medicines: \" + \"; \".join(meds))\n",
    "    if herbs:\n",
    "        lines.append(\"Suggested herbal treatments: \" + \"; \".join(herbs))\n",
    "    if nuts:\n",
    "        lines.append(\"Nutrition advice: \" + \"; \".join(nuts))\n",
    "    lines.append(\"Lifestyle recommendations: \" + \"; \".join(life))\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fa106c5-0b8e-4ff8-b432-2cdef0162126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ready. To use the advisor interactively, run:\n",
      "\n",
      "run_advisor_interactive()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Interactive advisor function\n",
    "def run_advisor_interactive():\n",
    "    print(\"=== Health Advisor ===\")\n",
    "    name = input(\"Patient name (optional): \").strip()\n",
    "    age = input(\"Age (optional): \").strip()\n",
    "    gender = input(\"Gender (optional): \").strip()\n",
    "    symptoms = input(\"Describe symptoms and signs (brief): \").strip()\n",
    "    if not symptoms:\n",
    "        print(\"No symptoms provided. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    context_parts = [symptoms]\n",
    "    if age:\n",
    "        context_parts.append(\"age \" + age)\n",
    "    if gender:\n",
    "        context_parts.append(\"gender \" + gender)\n",
    "    \n",
    "    context = \" \".join(context_parts)\n",
    "\n",
    "    matches = match_diagnoses(context, top_k=5)\n",
    "    if not matches:\n",
    "        print(\"No probable diagnosis found. Consider consulting a clinician.\")\n",
    "        return\n",
    "\n",
    "    composed = []\n",
    "    for diag, score in matches[:3]:\n",
    "        composed.append(compose_advice(diag, score))\n",
    "    final_text = \"\\n\\n---\\n\\n\".join(composed)\n",
    "\n",
    "    print(\"\\n===== HEALTH ADVICE =====\\n\")\n",
    "    print(final_text)\n",
    "\n",
    "print(\"\\nReady. To use the advisor interactively, run:\\n\\nrun_advisor_interactive()\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4eae3cbe-ac3f-403a-84e9-bc2730595b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Health Advisor ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Patient name (optional):  he\n",
      "Age (optional):  \n",
      "Gender (optional):  \n",
      "Describe symptoms and signs (brief):  headache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medicine lookup: diagnosis='Hyperthyroidism (chronic)', key_col='Disease', val_col='Medicine'\n",
      "Found medicine results: []\n",
      "Herbal lookup: diagnosis='Hyperthyroidism (chronic)', key_col='Disease', val_col='Herb'\n",
      "Found herbal results: []\n",
      "Nutrition lookup: diagnosis='Hyperthyroidism (chronic)', key_col='Disease', val_col='Recommended_Food'\n",
      "Found nutrition results: []\n",
      "Medicine lookup: diagnosis='Migraine or hypertension', key_col='Disease', val_col='Medicine'\n",
      "Found medicine results: ['Ranitidine', 'Hydroxychloroquine', 'Paracetamol', 'Omeprazole', 'Diazepam', 'Ciprofloxacin', 'Hydrochlorothiazide', 'Azithromycin', 'Salbutamol', 'Chloroquine', 'Metformin', 'Aspirin', 'Ibuprofen', 'Metronidazole', 'Insulin']\n",
      "Herbal lookup: diagnosis='Migraine or hypertension', key_col='Disease', val_col='Herb'\n",
      "Found herbal results: ['Garlic', 'Ashwagandha', 'Licorice Root', 'Ginger Root', 'Sage', 'Turmeric', 'Coriander', 'Lemongrass', 'Basil', 'Holy Basil', 'Neem']\n",
      "Nutrition lookup: diagnosis='Migraine or hypertension', key_col='Disease', val_col='Recommended_Food'\n",
      "Found nutrition results: ['Sweet Potato', 'Apple', 'Brown Rice', 'Ginger', 'Oranges', 'Fish', 'Green Tea', 'Beans', 'Spinach', 'Oats', 'Banana', 'Tomato', 'Avocado']\n",
      "Medicine lookup: diagnosis='Leukemia (acute)', key_col='Disease', val_col='Medicine'\n",
      "Found medicine results: []\n",
      "Herbal lookup: diagnosis='Leukemia (acute)', key_col='Disease', val_col='Herb'\n",
      "Found herbal results: []\n",
      "Nutrition lookup: diagnosis='Leukemia (acute)', key_col='Disease', val_col='Recommended_Food'\n",
      "Found nutrition results: []\n",
      "\n",
      "===== HEALTH ADVICE =====\n",
      "\n",
      "Most likely diagnosis: Hyperthyroidism (chronic) (score=0.347)\n",
      "Lifestyle recommendations: Maintain healthy diet\n",
      "\n",
      "---\n",
      "\n",
      "Most likely diagnosis: Migraine or hypertension (score=0.300)\n",
      "Recommended medicines: Ranitidine; Hydroxychloroquine; Paracetamol; Omeprazole; Diazepam\n",
      "Suggested herbal treatments: Garlic; Ashwagandha; Licorice Root; Ginger Root; Sage\n",
      "Nutrition advice: Sweet Potato; Apple; Brown Rice; Ginger; Oranges\n",
      "Lifestyle recommendations: Follow prescribed care plan\n",
      "\n",
      "---\n",
      "\n",
      "Most likely diagnosis: Leukemia (acute) (score=0.266)\n",
      "Lifestyle recommendations: Maintain balanced diet; Regular exercise; Get routine checkups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14307/18853937.py:21: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  contains_matches = df[df[key_col].astype(str).str.lower().str.contains(word, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:21: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  contains_matches = df[df[key_col].astype(str).str.lower().str.contains(word, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:21: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  contains_matches = df[df[key_col].astype(str).str.lower().str.contains(word, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/3268562806.py:118: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  partial_matches = cp_df[cp_df[diag_col].astype(str).str.lower().str.contains(word, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:21: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  contains_matches = df[df[key_col].astype(str).str.lower().str.contains(word, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:21: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  contains_matches = df[df[key_col].astype(str).str.lower().str.contains(word, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:21: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  contains_matches = df[df[key_col].astype(str).str.lower().str.contains(word, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/18853937.py:29: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset = df[df[c].astype(str).str.lower().str.contains(diag_l, na=False)]\n",
      "/tmp/ipykernel_14307/3268562806.py:118: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  partial_matches = cp_df[cp_df[diag_col].astype(str).str.lower().str.contains(word, na=False)]\n"
     ]
    }
   ],
   "source": [
    "run_advisor_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f0e0b-7354-4e25-9737-d1442004a87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
